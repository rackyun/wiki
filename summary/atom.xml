<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[个人技术图谱梳理]]></title>
  <link href="https://www.rackyun.ml:8810/wiki/summary/atom.xml" rel="self"/>
  <link href="https://www.rackyun.ml:8810/wiki/summary/"/>
  <updated>2018-03-22T15:06:56+08:00</updated>
  <id>https://www.rackyun.ml:8810/wiki/summary/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im">MWeb</generator>

  
  <entry>
    <title type="html"><![CDATA[TCP配置调优]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15223942546537.html"/>
    <updated>2018-03-30T15:17:34+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15223942546537.html</id>
    <content type="html"><![CDATA[
<hr/>

<p>几个服务器配置调优点：<br/>
- TCP快速打开TFO<br/>
- 禁用慢启动重启<br/>
- 增大TCP的初始拥塞窗口</p>

<h2 id="toc_0">TFO</h2>

<h3 id="toc_1">TCP三次握手</h3>

<p><img src="media/15217906741904/15222279127478.jpg" alt=""/></p>

<ul>
<li>SYN
客户端随机选择一个序列号x，并发送一个SYN分组，其中可能还包括了其他TCP标志和选项，比如图中的mss=1024的选项。</li>
<li>SYN ACK
服务端给x+1设置为确认号，并选择自己的一个随机序列号y，追加自己的标志和选项</li>
<li>ACK
客户端给y+1作为握手期间的最后一个ack分组。之后就可以发送数据，服务端需要等待收到ack分组之后才能发送数据。
为什么需要3次握手，主要是要初始化Sequence Number的初始值。通讯双方要互相通知对方自己初始化的Sequence Number，所以叫SYN(Synchronize Sequence Numbers)。
每次新建一个TCP连接都需要经历三次握手过程，带来的延迟使得新建连接要付出很大的成本。</li>
</ul>

<h3 id="toc_2">TCP Fast Open</h3>

<p>TFO是在原始TCP基础上进行扩展的协议，它基于TCP的改良之处是在三次握手期间可以进行应用数据的传输。Linux内核从3.7版本开始支持，3.13以后默认启动。但是nginx默认并没有启动TFO。<br/>
<img src="media/15217906741904/15222465200081.jpg" alt=""/></p>

<h4 id="toc_3">运行原理</h4>

<p>第一次连接<br/>
1. 客户端发送SYN包，包尾加一个FOC请求，只有4个字节<br/>
2. 服务器端收到FOC请求，验证后会根据来源IP地址生成cookie(8个字节)，将这个Cookie加载到SYN + ACK包的末尾发送回去<br/>
3. 客户端缓存获取到的Cookie，给下次连接使用<br/>
第二次连接<br/>
1. 客户端发送SYN包，后面带上缓存的cookie，然后就是正式发送的数据<br/>
2. 服务器端验证Cookie正确，将数据交给上层应用处理得到相应的结果，然后在发送SYN+ACK时，不再等待客户端的ACK确认，即开始发送相应的数据。</p>

<p><img src="media/15217906741904/15222471221701.jpg" alt=""/></p>

<h4 id="toc_4">Cookie</h4>

<p>TFO的Cookie是用来快速打开的关键，所以有一些限制是需要遵守的：<br/>
1. Cookie的长度必须是偶数，且长度是0或者介于4~16<br/>
   第一阶段CookieOpt=Nil的SYN包，这个时候还没有Cookie所以长度是0<br/>
2. TFO Server生成Cookie需要快速，生成的Cookie有时效性<br/>
   - 简单的实现就是直接将客户端地址+Key进行AES-128加密，然后截断为64位传给客户端，下次直接对客户端的IP进行同样的操作，然后对比结果<br/>
   - 时效性的话，可以定期更换Server端的Key，这样以前的Cookie都失效<br/>
3. TFO Client Cookie处理<br/>
   - TFO客户端应该将Cookie保存下来，如果是多实例的客户端，需要针对每个客户端都应该保存一份<br/>
   - TFO客户端尽量将MSS也保存下来，这样下次使用的时候第一次就可以知道传多少数据合适，不需要等ACK传回MSS才能知道</p>

<h4 id="toc_5">收益</h4>

<p><img src="media/15217906741904/15222476455357.jpg" alt=""/></p>

<p>除了页面加载变快改善了用户体验之外，TFO给服务器也带来了一些好处。由于每个请求都节省了一个RRT，相应地也减少了服务器端Cpu消耗。</p>

<h2 id="toc_6">慢启动重启</h2>

<h3 id="toc_7">TCP的流量控制</h3>

<p>流量控制是一种预防发送端过多的向接收端发送数据的机制。<br/>
第一次建立连接时，两端都会使用自身系统的默认设置来发送rwnd(receiver window)，告诉对方自己最大能保存多少数据。如果一端跟不上数据传输，它可以向发送端通告一个较小的窗口，如果窗口为零，则意味必须由应用层先清空缓冲区，才能继续接收数据，此时发送端暂停发送的等待接收端通过ACK重新发送一个新的窗口值。由于这个ACK报文段可能会丢失且不会被确认，有可能造成传输死锁。因此发送端会有一个零窗口探测定时器，当接收方的rwnd为0时，定时器开始启动，每隔一段时间，发送端会主动发送一个零窗口探测报文段，通过对端ACK来得知最新的窗口值。</p>

<h3 id="toc_8">慢启动</h3>

<p>虽然流量控制可以避免发送端过多的向接收端发送数据，但是无法避免网络过载。rwnd只反映了服务器个体的情况，无法反映网络整体的情况。<br/>
为了避免网络过载的问题，慢启动引入了<strong>拥塞窗口(cwnd，congestion window)</strong>，用来表示发送方在得到接收方确认(ACK)之前，最大允许传输的未经确认的数据。客户端与服务端之间最大可以传输(未经ACK确认的)数据量取rwnd和cwnd变量中的最小值。cwnd与rwnd不同，它是发送方的一个内部参数，无需通知给接收方，其初始值比较小(<strong>Linux 3.0之前默认initCwnd为 3个TCP段，3.0之后调整为10</strong>)。</p>

<p><img src="media/15222900353019/15223057806579.jpg" alt=""/></p>

<ol>
<li>发送端刚开始用系统默认initCwnd发送报文段</li>
<li>在每收到一个新的报文段的ACK后，将cwnd增加至多一个MSS的数值(假设一开始窗口是1个段，发送一个确认一个，增长为2；发送2个，确认2个，增长为4...因此窗口是指数增长的）</li>
<li>因为cwnd是指数增长的，为了防止后期增长太快，需要另外一个变量--慢启动阈值ssthresh，当cwnd==ssthresh是，要预防拥塞的产生，开始执行拥塞预防算法，cwnd按线性增长</li>
<li>当发送端发现丢包，TCP即认为发生了网络拥塞，开始启动拥塞预防算法，调整cwnd大小，以避免造成更多的包丢失。</li>
</ol>

<blockquote>
<p>丢包有两种情况：<br/>
   1. 等到RTO，重传数据包。TCP认为这种情况太糟糕，反应比较强烈。<br/>
      - ssthresh = cwnd/2<br/>
      - cwnd重置为1<br/>
      - 进入慢启动过程<br/>
   2. 快速重传，即在收到3个duplicate ACK时就开始重传，而不等待RTO，TCP认为能收到3个duplicate ACK说明网络不是那么糟糕，<br/>
      - TCP Tahoe的实现跟RTO超时时相同<br/>
      - TCP Reno的实现是<br/>
            - cwnd = cwnd/2<br/>
            - ssthresh = cwnd<br/>
            - 进入快速恢复算法<br/>
    快速恢复算法<br/>
       - cwnd = ssthresh + 3*MSS（3的意思是确认有3个数据包被收到了）<br/>
       - 重传Duplicate ACK指定的数据包<br/>
       - 如果再收到Duplicate ACK，那么cwnd = cwnd + 1<br/>
       - 如果收到新的ACK，那么cwnd设置初始的ssthresh，开始进入拥塞避免阶段</p>
</blockquote>

<p>PRR(比例降速)是google发布的一个降窗算法，目的是改进丢包之后恢复的速度。根据谷歌的测试，实现新的算法之后，因丢包造成的平均连接延迟减少了3%~10%。PRR现在是Linux 3.2+内核默认的拥塞预防算法。<br/>
PRR算法的特点：<br/>
1. 不再减半，而是完全根据拥塞算法计算出的ssthresh，调整窗口逼近它；<br/>
2. 执行的过程不再受当前的in_flight控制，而是根据快速重传以来的发送/接收ACK的总数量将窗口按照比例的方式逼近ssthresh；<br/>
3. 如果窗口降到ssthresh一下，算法执行慢启动将其拉升至ssthresh附近；<br/>
4. 快速恢复阶段，最多还可以发送多少数据，不再限定为1，而是取决于“当前收到的ACK/SACK总量，发出数据总量，窗口与ssthresh的关系”。</p>

<h3 id="toc_9">慢启动重启(Slow-Start Restart)</h3>

<p>慢启动重启机制会在连接空闲一段时间后重置连接的拥塞窗口。在连接空闲的同时，网络状况可能发生了变化，为了避免拥塞，应该将拥塞窗口重置会“安全的”默认值。<br/>
SSR对于那些会出现突发空闲的长周期TCP连接（比如HTTP的keep-alive连接）有很大影响。因此，建议在服务器端禁用SSR。在linux上可以通过以下命令来查看和禁用SSR<br/>
<code><br/>
    sysctl net.ipv4.tcp_slow_start_after_idle<br/>
    sysctl -w net.ipv4.tcp_slow_start_after_idle=0<br/>
</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP与UDP]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15217906741904.html"/>
    <updated>2018-03-23T15:37:54+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15217906741904.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">TCP</h2>

<p>TCP提供一种面向连接的、可靠的字节流服务。<br/>
面向连接意味着TCP在彼此交换数据之前必须先建立一个TCP连接，通讯结束之后要断开连接。<br/>
可靠性是通过下列方式来提供的：<br/>
- 应用数据被TCP分割成最适合传输的数据块（报文段segment）<br/>
- TCP发送一个段后，启动一个定时器，等待目标端确认收到这个报文段，如果不能及时收到一个确认，将重发这个报文段<br/>
- 当TCP收到发自另一端的数据，将发送一个确认。这个确认不是立即发送，通常会延迟几分之一秒<br/>
- TCP将保持它首部和数据的校验和。如果收到段的校验和有差错，TCP将丢弃这个报文段（希望发送端超时并重发）<br/>
- 由于IP数据报到达可能会时序，TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层<br/>
- IP数据报可能发送重复，TCP接收端会丢弃重复的数据<br/>
- TCP提供了流量控制。TCP连接的每一方都有固定大小的缓冲空间，接收端只允许另一端发送接收端缓冲区能够接纳的数据。这将防止较快的主机导致较慢的主机缓冲区溢出。</p>

<h3 id="toc_1">TCP首部</h3>

<p><img src="media/15217906741904/15222190968688.jpg" alt=""/></p>

<ul>
<li>源端口、目标端口：16位表示，用于传输层多路复用</li>
<li>序列号：表示本报文段所发送数据的第一个字节的编号，在TCP连接中所发送的字节流的每一个字节都会按顺序编号。32位表示，每2<sup>32个字节，就会出现序列号回绕，再次从0开始。(TODO</sup> 如何区分两个相同序列号的不同TCP报文段)</li>
<li>确认号：表示接收方期望收到发送方下一个报文段的第一个字节数据编号。</li>
<li>TCP首部长度：由于TCP首部包含一个可变长度的选项部分，因此需要这样一个值表示这个TCP首部到底有多长，32位。</li>
<li>URG：表示本报文段中是否包含紧急数据，URG=1时后面的紧急指针字段才有效。</li>
<li>ACK：表示前面的确认号字段是否有效。TCP规定，连接建立后，ACK必须为1。</li>
<li>PSH：表示接收端是否应该立即将数据推送给上层。</li>
<li>RST：表示连接出现严重错误，必须释放连接，然后重新建立连接。或者说明上次发送的数据有问题，接收端拒绝响应。</li>
<li>SYN：表示是否是请求建立连接或者同意建立连接的报文。在建立连接时使用，用来同步序号。当SYN=1，ACK=0时，表示这是一个请求建立连接的报文段；当SYN=1，ACK=1时，表示对方同意建立连接。</li>
<li>FIN：表示数据是否发送完毕。FIN=1，相当于告诉对方，数据已经发送完毕，可以释放连接。</li>
<li>窗口大小：告诉对方，从本报文段的确认号开始允许对方发送的数据量。</li>
<li>校验和：</li>
<li>紧急指针：标记紧急数据在数据段中的位置</li>
<li>选项部分：选项部分的最大长度为40个字节
选项部分的应用

<ol>
<li>MSS：最大报文段长度指数据字段的最大长度(不包含TCP首部长度)。MSS指发送端期望对端发送TCP报文段时数据字段的长度，如未填写，默认采用536字节。MSS只出现在SYN报文中。</li>
<li>窗口扩大选项：TCP首部的窗口大小长度是16位，最大值是65535。随着时延和带宽较大的通讯产生，需要更大的窗口来满足性能和吞吐率，所以产生了这个窗口扩大选项</li>
<li>SACK选择确认项：</li>
<li>时间戳选项：可以用来计算RTT，也可以防止回绕序号。</li>
<li>NOP(NO-Operation)：选项中要求每种选项长度都必须是4字节的倍数，不足的则用NOP填充。也可以用来分割不同的选项字段。</li>
</ol></li>
</ul>

<h3 id="toc_2">三次握手</h3>

<p><img src="media/15217906741904/15222279127478.jpg" alt=""/></p>

<p><img src="media/15217906741904/15222301277225.jpg" alt=""/></p>

<ul>
<li>SYN
客户端随机选择一个序列号x，并发送一个SYN分组，其中可能还包括了其他TCP标志和选项，比如图中的mss=1024的选项。</li>
<li>SYN ACK
服务端给x+1设置为确认号，并选择自己的一个随机序列号y，追加自己的标志和选项</li>
<li>ACK
客户端给y+1作为握手期间的最后一个ack分组。之后就可以发送数据，服务端需要等待收到ack分组之后才能发送数据。
为什么需要3次握手，主要是要初始化Sequence Number的初始值。通讯双方要互相通知对方自己初始化的Sequence Number，所以叫SYN(Synchronize Sequence Numbers)。
每次新建一个TCP连接都需要经历三次握手过程，带来的延迟使得新建连接要付出很大的成本。</li>
</ul>

<h4 id="toc_3">TCP Fast Open</h4>

<p>TFO是在原始TCP基础上进行扩展的协议，它基于TCP的改良之处是在三次握手期间可以进行应用数据的传输。Linux内核从3.7版本开始支持，3.13以后默认启动。<br/>
<img src="media/15217906741904/15222465200081.jpg" alt=""/></p>

<h5 id="toc_4">运行原理</h5>

<p>第一次连接<br/>
1. 客户端发送SYN包，包尾加一个FOC请求，只有4个字节<br/>
2. 服务器端收到FOC请求，验证后会根据来源IP地址生成cookie(8个字节)，将这个Cookie加载到SYN + ACK包的末尾发送回去<br/>
3. 客户端缓存获取到的Cookie，给下次连接使用<br/>
第二次连接<br/>
1. 客户端发送SYN包，后面带上缓存的cookie，然后就是正式发送的数据<br/>
2. 服务器端验证Cookie正确，将数据交给上层应用处理得到相应的结果，然后在发送SYN+ACK时，不再等待客户端的ACK确认，即开始发送相应的数据。</p>

<p><img src="media/15217906741904/15222471221701.jpg" alt=""/></p>

<h5 id="toc_5">Cookie</h5>

<p>TFO的Cookie是用来快速打开的关键，所以有一些限制是需要遵守的：<br/>
1. Cookie的长度必须是偶数，且长度是0或者介于4~16<br/>
   第一阶段CookieOpt=Nil的SYN包，这个时候还没有Cookie所以长度是0<br/>
2. TFO Server生成Cookie需要快速，生成的Cookie有时效性<br/>
   - 简单的实现就是直接将客户端地址+Key进行AES-128加密，然后截断为64位传给客户端，下次直接对客户端的IP进行同样的操作，然后对比结果<br/>
   - 时效性的话，可以定期更换Server端的Key，这样以前的Cookie都失效<br/>
3. TFO Client Cookie处理<br/>
   - TFO客户端应该将Cookie保存下来，如果是多实例的客户端，需要针对每个客户端都应该保存一份<br/>
   - TFO客户端尽量将MSS也保存下来，这样下次使用的时候第一次就可以知道传多少数据合适，不需要等ACK传回MSS才能知道</p>

<h5 id="toc_6">收益</h5>

<p><img src="media/15217906741904/15222476455357.jpg" alt=""/></p>

<p>除了页面加载变快改善了用户体验之外，TFO给服务器也带来了一些好处。由于每个请求都节省了一个RRT，相应地也减少了服务器端Cpu消耗。</p>

<h3 id="toc_7">四次挥手</h3>

<p><img src="media/15217906741904/15222303125310.png" alt=""/></p>

<p>首先进行关闭的一方发送第一个FIN执行主动关闭，此时应用程序不能<br/>
为什么需要4次挥手，因为TCP是全双工的，所以发送方和接收方都要FIN和ACK，只不过有一方是主动的，所以看上去就成了所谓的4次挥手。</p>

<h3 id="toc_8">IP分片</h3>

<p>IP层接收到一份需要发送的IP数据报时，它要判断向本地哪个接口发送数据，并查询该接口获得其MTU。IP把MTU与数据报长度进行比较，如果需要则进行分片。分片可以发生在原始发送端主机上，也可能发生在中间路由器上。<br/>
把一份IP数据报分片以后，只有到达目的地才进行重新组装。重新组装由目的端的IP层来完成，其目的是使分片和重新组装过程对传输层(TCP和UDP)是透明的，除了某些可能的越级操作外。已经分片过的数据报有可能会再次进行分片(可能不止一次)。IP首部中包含的数据为分片和重新组装提供了足够的信息。选择路由时每个分组(一个分片都成为一个分组)可以与其他分组独立。</p>

<p>IP层本身没有超时重传的机制--由更高层来负责超时和重传。当来自TCP报文段的某一片丢失后，TCP在超时后会重发整个TCP报文段。没有办法只重传数据报中的一个数据报片。事实上，如果对数据报分片的是中间路由器，而不是起始端，那么起始端系统就无法知道数据报时如何分片的。因此一般要避免在IP层分片。任何运输层首部只出现在第1片数据中。</p>

<h4 id="toc_9">MTU</h4>

<p>最大传输单元(Maximum Transmission Unit)是指一种通讯协议的某一层面上所能通过的最大数据包大小(单位为字节)。IP协议中，一条IP传输路径的“路径最大传输单元”被定义为从源地址到目标地址所经过“路径”上的所有IP跳的最大传输单元的最小值。</p>

<h5 id="toc_10">路径最大传输单元发现方法</h5>

<p>这是一种确定两个IP主机之间路径最大传输单元的技术。在这项技术中，源地址将设置数据报的DF(Don&#39;t Fragment，不要分片)标记位，再逐渐增大发送的数据报的大小--路径上任何需要将分组进行分片的设备都会将这种数据报丢弃并返回一个“数据报过大”的ICMP响应到源地址--这样，源主机就“获取”到了不用进行分片就能通过这条路径的最大传输单元。<br/>
不幸的是，越来越多的网络封杀了 ICMP 的传输（譬如说为了防范 DoS 攻击）——这使得路径最大传输单元发现方法不能正常工作，其常见表现就是一个连接在低数据流量的情况下可以正常工作，但一旦有大量数据同时发送，就会立即挂起（例如在使用 IRC 的时候，客户会发现在发送了一个禁止 IP 欺骗的 ping 之后就得不到任何响应了，这是因为该连接被大量的欢迎消息堵塞了）。而且，在一个使用因特网协议的网络中，从源地址到目的地址的“路径”常常会为了响应各种各样的事件（负载均衡、拥塞、断电等等）而被动态地修改——这可能导致路径最大传输单元在传输过程中发生改变——有时甚至是反复的改变。其结果是，在主机寻找新的可以安全工作的最大传输单元的同时，更多的分组被丢失掉了。</p>

<p>对于时下大多数使用以太网的局域网来说，最大传输单元的值是 1,500 字节。但是像 PPPoE 这样的系统会减小这个数值，通常是1492（= 1500 - 2（PPP）- 6（PPPoE））。</p>

<h4 id="toc_11">MSS</h4>

<p>最大分段大小，是TCP数据包每次能够传输的最大数据分段，TCP默认的MSS值为536。为了达到最佳的传输效能TCP协议在建立连接的时候通常需要协商双方的MSS值，这个值TCP协议在实现的时候往往用MTU值代替(需要减去IP数据包头的20Byte和TCP数据段的包头20Byte)所以往往MSS为1460.通讯双方会根据双方提供的MSS值的最小值确定为这次连接的MSS值。<br/>
MSS协商过程：<br/>
TCP Client发出SYN报文，其中option选项填充的MSS字段一般为(MTU - IP头size - TCP头size)，同样TCP Server收到SYN报文后，会发送SYN + ACK报文应答，option选项填充的mss字段也为(MTU - IP头size - TCP头size)；协商双方会比较SYN和SYN+ACK报文中mss字段的大小，选择较小的mss作为TCP分片的大小。</p>

<h2 id="toc_12">UDP(User Datagram Protocol)</h2>

<h3 id="toc_13">UDP头</h3>

<p><img src="media/15217906741904/15220506658472.jpg" alt=""/></p>

<p>UDP仅仅是在IP层之上通过嵌入了应用程序的源端口和目标端口，提供了一个“应用程序多路复用”机制。<br/>
因此可以决定了UDP的特点：<br/>
- 不保证消息交付。不确认，不重传，无超时。<br/>
- 不保证交付顺序。没有包序号，不重排，不会发生队首阻塞。<br/>
- 不跟踪连接状态。不必建立连接或重启状态机。<br/>
- 不需要拥塞控制。不内置客户端或网络反馈机制。</p>

<p>UDP数据报必须封装在IP分组中，应用程序必须读取完整的信息，因此数据报不能分片。</p>

<h3 id="toc_14">NAT</h3>

<p>网络地址转换，也叫网络掩蔽或IP掩蔽，是一种在IP数据报通过路由器或防火墙时重写来源IP地址或目标IP地址的技术。这种技术被普遍使用在有多台主机但只通过一个公有IP地址访问因特网的私有网络中。根据规范，路由器是不能这样工作的，但是它的确是一个方便且得到了广泛应用的技术。</p>

<h3 id="toc_15">nginx的udp负载均衡</h3>

<p>nginx的负载均衡使用的反向代理模式</p>

<h3 id="toc_16">TCP协议的困境</h3>

<p>从上个世纪90年代互联网兴起一直到现在，大部分的互联网流量传输只使用了网络协议。使用IPV4进行路由，使用TCP进行连接层面的流量控制，使用SSL/TLS协议实现传输安全，使用DNS进行域名解析，使用HTTP进行应用层的传输。<br/>
近三十年来，这几个协议都发展缓慢。TCP主要是拥塞控制算法的改进，SSL/TLS之前几个小版本的改动主要是密码套件的升级。TLS1.3有飞跃式的发展但是目前为止还未正式发布，IPV4到IPV6有了巨大的进步，DNS增加了一个安全的DNSSEC，然而和IPV6一样部署进度缓慢。<br/>
传统协议面临几个问题：<br/>
1. 协议历史悠久导致中间设备僵化<br/>
2. 依赖操作系统的实现导致协议本身僵化<br/>
3. 建立连接的握手延迟大<br/>
4. 队首阻塞</p>

<h4 id="toc_17">中间设备的僵化</h4>

<p>TCP协议本来是支持端口、选项及特性的增加和修改。但是由于TCP协议用的太久了，很多中间设备，包括防火墙、NAT网关、整流器等出现了一些约定俗成的动作。比如有些防火墙只允许通过80和443，不开放其他端口。NAT网关在网络地址转换时重写传输层头部，有可能导致双方无法使用新的传输格式。整流器和中间代理有时候出于安全的需要，会删除一些它们不认识的选项字段，从而导致特性修改失败。</p>

<h4 id="toc_18">依赖于操作系统的实现导致协议僵化</h4>

<p>TCP是由操作系统在内核协议栈层面实现的，一般情况下应用程序只能使用，不能直接修改。无论是服务器端还是PC端操作系统的升级都非常麻烦、滞后。导致TCP特性更新，很难快速推广。比如TFO(TCP Fast Open)，虽然2013年就被提出了，但是Windows很多版本不支持(该特性需要客户端和服务端同时支持)。</p>

<h4 id="toc_19">建立连接的握手延迟大</h4>

<ol>
<li>TCP需要三次握手才能建立连接</li>
<li>TLS完全握手需要至少2个RTT才能建立，简化握手需要1个RTT</li>
</ol>

<h4 id="toc_20">队首阻塞</h4>

<p>TCP使用序列号来标识数据包的顺序，必须按照顺序处理，如果前面的数据报丢失，后面的数据就无法交给应用进程处理，需要阻塞等待数据重传。<br/>
HTTP 1.1 通过pipelining管道技术实现一次性发送多个请求，以期提高吞吐和性能。但是要求必须按照发送请求的顺序返回，如果第一个请求被堵塞了，则后面的请求即使处理完毕也需要等待。<br/>
HTTP/2 使用了数据分帧的办法，多个请求复用一个TCP连接，将每个请求响应都分拆为若干个frame发送，这样即使一个请求被阻塞了，也不会影响其他请求。但是HTTP 2 over TCP无法解决传输层的队首阻塞问题。</p>

<h5 id="toc_21">TLS协议的队首阻塞</h5>

<p>Record是TLS协议处理的最小单位，最大不能超过16K，一些服务器比如Nginx默认的大小就是16K。由于一个Record必须经过数据一致性校验才能进行加解密，所以record中一部分丢失会导致整个Record无法处理。</p>

<h3 id="toc_22">QUIC(Quick Udp Internet Connection)快速UDP互联网连接</h3>

<p>Quic相对于现在的http2+tls+tcp协议有如下的优势：<br/>
1. 减少了TCP三次握手及TLS握手时间<br/>
2. 改进的拥塞控制<br/>
3. 避免队首阻塞的多路复用<br/>
4. 连接迁移<br/>
5. 前向冗余纠错</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[代理模式与装饰器模式]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15217710456795.html"/>
    <updated>2018-03-23T10:10:45+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15217710456795.html</id>
    <content type="html"><![CDATA[
<p>代理模式和装饰器模式的关注点不同，代理模式关注于控制对对象的访问，可以对客户隐藏一个对象的具体信息，生活中的代理就是客户只跟代理人打交道，由代理人与上游供应商打交道；装饰器关注对一个对象的动态增强，一般使用装饰器模式的时候会将原始对象作为参数传递给装饰器的构造器。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java对象的内存结构]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15213446875839.html"/>
    <updated>2018-03-18T11:44:47+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15213446875839.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[需要进一步研究的关键点]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15213411103122.html"/>
    <updated>2018-03-18T10:45:10+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15213411103122.html</id>
    <content type="html"><![CDATA[
<hr/>

<h3 id="toc_0"><a href="15212921864702.html">Java内存区域</a></h3>

<ul>
<li>操作系统对一个进程里的线程数有限制，经验值在3000~5000 </li>
<li>随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也逐渐变得不是那么“绝对”了。</li>
<li>元空间</li>
<li>Survivor空间的分配担保</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java内存区域]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15212921864702.html"/>
    <updated>2018-03-17T21:09:46+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15212921864702.html</id>
    <content type="html"><![CDATA[
<hr/>

<ul>
<li>
<a href="#toc_0">线程私有内存</a>
<ul>
<li>
<a href="#toc_1">程序计数器</a>
</li>
<li>
<a href="#toc_2">虚拟机栈</a>
</li>
<li>
<a href="#toc_3">本地方法栈</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">所有线程共享数据区</a>
<ul>
<li>
<a href="#toc_5">Java堆</a>
</li>
<li>
<a href="#toc_6">方法区</a>
<ul>
<li>
<a href="#toc_7">运行时常量池</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_8">直接内存</a>
</li>
</ul>


<p><img src="media/15212921864702/15212938305753.jpg" alt=""/></p>

<p>Java程序运行时会将其管理的内存换分为几个区域，如图所示。</p>

<h2 id="toc_0">线程私有内存</h2>

<h3 id="toc_1">程序计数器</h3>

<p>程序计数器是一块较小的内存区域，记录了当前线程执行的字节码行数指示器。由于JVM的多线程是通过线程轮流切换并分配处理器时间片的方式来实现的，同一时间一个处理器核只能执行一个线程中的指令，因此为了线程切换以后能恢复到正确的位置执行，因此每个线程需要有一个独立的程序计数器。<br/>
如果线程执行的是Java方法，程序计数器中存储的是虚拟机字节码指令的地址；如果执行的是Native方法，程序计数器值为空(Undefined)。这块区域是唯一一块Java虚拟机规范中没有规定任何OutOfMemoryError的区域。</p>

<h3 id="toc_2">虚拟机栈</h3>

<p>虚拟机栈生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型，每个方法在执行时都会创建一个栈帧(Stack frame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息，一个方法从调用到执行完成的过程，对应着一个栈帧在虚拟机栈中入栈、出栈的过程。<br/>
局部变量表存储了编译期间可知的基本数据类型、对象引用类型、returnAddress类型(出口地址，指向了一个字节码指令的地址)。其中64位的long和dubbo类型的数据会占用两个局部变量空间(slot)，其他类型的数据只占用1个。局部变量表所需的内存空间在编译期间分配完成，当进入一个方法时，这个方法所需的局部变量表空间是完全确定的，方法执行过程中不会发生变化。<br/>
Java虚拟机规范中关于该区域规定了两种异常情况：1.如果线程请求的栈深度大于虚拟机所允许的深度，会抛出StackOverflowError异常(如递归层数过大)；2.如果虚拟机栈支持动态扩展，而扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。</p>

<p>HotSpot虚拟机中关于虚拟机栈的配置<br/>
-Xss 每个线程的栈大小 jdk5以后每个线程堆栈的默认大小为1M，之前为256k。<em>操作系统对一个进程里的线程数有限制，经验值在3000~5000</em>。小的应用如果栈不是很深，应该128k够用了，稍大的应用建议设置成256k，这个选项对性能影响比较大，需要进行严格的测试<br/>
-XX:ThreadStackSize 线程的栈的深度 (0 means use default stack size) [Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.]</p>

<h3 id="toc_3">本地方法栈</h3>

<p>本地方法栈功能与虚拟机栈相似，只是用于虚拟机调用Native方法。一些虚拟机(如hotSpot)将本地方法栈和虚拟机方法栈合二为一。与虚拟机栈一样，该区域抛出StackOverflowError和OutOfMemoryError两种异常。</p>

<h2 id="toc_4">所有线程共享数据区</h2>

<h3 id="toc_5">Java堆</h3>

<p>Java堆是虚拟机所管理内存中最大的一块，此区域唯一的目的就是存放对象实例，几乎所有对象都在堆上分配内存。<br/>
Java堆是垃圾收集器管理的主要区域，因此很多时候也被称为“GC堆”。现在收集器基本都采用分代收集算法，所以Java堆中还可以细分为：新生代和年老代；再细分一点有Eden、From Survivor、To Survivor空间。从内存分配的角度，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区。<br/>
根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。实现上堆空间可以是固定的，也可以是动态扩展的。实际上主流的虚拟机都是可以扩展的（通过-Xmx和-Xms控制）。如果在堆中没有足够内存完成实例分配，且堆也无法完成扩展，将会抛出OutOfMemoryError。</p>

<h3 id="toc_6">方法区</h3>

<p>方法区(Method Area)用于存储已经被虚拟机加载的类信息、常量、静态变量、及时编译器(JIT)编译后的代码等数据。另外一个别名叫做Non-heap。<br/>
HotSpot中曾经使用永久代来实现方法区，在永久代中使用分代收集器回收内存。1.7之前的永久代中还存储了常量池中的字符串对象，导致了一系列的性能问题和内存移除错误。1.7版本中将符号常量移动到Native Heap，将字符串常量移动到Java Heap。1.8版本取消了永久代，将数据区域转移到了一块与堆无关的本地内存区域即元空间(Metaspace VM for metadata)。<br/>
1.7 and earlier永久代配置 -XX:MaxPermSize 32位机器默认配置为64M 64位机器默认配置为85M。<br/>
该区域会抛出OutOfMemoryError。</p>

<h4 id="toc_7">运行时常量池</h4>

<ul>
<li>运行时常量池为方法区的一部分。</li>
<li>用于存储Class文件编译期间生成的各种字面量和符号引用，除此之外还会把翻译出来的直接引用也存储在运行时常量池中。</li>
<li>Java虚拟机对Class文件的每一个部分的格式都有严格的规定，每一个字节用于存储那种数据都必须符合规范上的要求才会被虚拟机认可、装载和执行。</li>
<li>运行时常量池相对于Class文件的常量池的另外一个重要特征是具有动态性，Java语言并不要求变量一定只有编译期才能产生，也就是说并非只有Class文件中的常量池的内容才能进入运行时常量池，运行时也可以将新的常量放入池中，这个特性利用较多的是String.intern()方法。</li>
<li>该区域会抛出OutOfMemoryError。</li>
</ul>

<h2 id="toc_8">直接内存</h2>

<ul>
<li>直接内存(Direct Memory)并不是虚拟机运行时数据区的一部分，也并不是Java虚拟机规范中定义的内存区域</li>
<li>JDK1.4中加入了NIO，引入了一种基于通道与缓冲区的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为对这块内存的引用进行操作。这样避免了数据在Java Heap和Native Heap中来回复制数据，提高了性能。</li>
<li>直接内存不受Java堆大小的限制，直接内存加其他区域内存综合大于物理内存限制时会发生OutOfMemoryError</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GC算法知识整理]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15212852694452.html"/>
    <updated>2018-03-17T19:14:29+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15212852694452.html</id>
    <content type="html"><![CDATA[
<hr/>

<ul>
<li>
<a href="#toc_0">对象引用分析</a>
<ul>
<li>
<a href="#toc_1">引用计数法</a>
</li>
<li>
<a href="#toc_2">可达性分析</a>
</li>
<li>
<a href="#toc_3">引用类型</a>
</li>
<li>
<a href="#toc_4">finalize方法</a>
</li>
<li>
<a href="#toc_5">回收方法区</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">垃圾收集算法</a>
<ul>
<li>
<a href="#toc_7">标记-清除算法</a>
</li>
<li>
<a href="#toc_8">复制算法</a>
</li>
<li>
<a href="#toc_9">标记-整理算法</a>
</li>
<li>
<a href="#toc_10">分代收集算法</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">HotSpot的算法实现</a>
<ul>
<li>
<a href="#toc_12">枚举根节点</a>
</li>
<li>
<a href="#toc_13">安全点</a>
</li>
<li>
<a href="#toc_14">安全区域</a>
</li>
</ul>
</li>
<li>
<a href="#toc_15">垃圾收集器</a>
<ul>
<li>
<a href="#toc_16">Serial收集器</a>
</li>
<li>
<a href="#toc_17">ParNew(Parallel New)收集器</a>
</li>
<li>
<a href="#toc_18">Parallel Scavenge收集器(吞吐量优先收集器)</a>
</li>
<li>
<a href="#toc_19">Serial Old收集器</a>
</li>
<li>
<a href="#toc_20">Parallel Old收集器</a>
</li>
<li>
<a href="#toc_21">CMS收集器</a>
</li>
<li>
<a href="#toc_22">G1(Gebage-First)收集器</a>
<ul>
<li>
<a href="#toc_23">对象分配策略</a>
</li>
<li>
<a href="#toc_24">G1的两种GC模式</a>
<ul>
<li>
<a href="#toc_25">G1 Young GC</a>
</li>
<li>
<a href="#toc_26">G1 Mixed GC</a>
</li>
</ul>
</li>
<li>
<a href="#toc_27">G1的收集过程</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>垃圾回收算法主要解决三个问题：<br/>
1. 哪些内存需要回收<br/>
2. 什么时候回收<br/>
3. 如何回收</p>

<h2 id="toc_0">对象引用分析</h2>

<h3 id="toc_1">引用计数法</h3>

<p>引用计数法通过给每个对象添加一个引用计数器：被引用时，计数器加1，引用失效时，计数器减1；当计数器为0时表示对象不再被引用。这种实现简单，判定效率高，大部分情况下它是一种不错的算法，在微软的COM、使用ActionScript3的FlashPlayer、Python等语言中使用了引用计数法来管理内存。但是在Java虚拟机中没有使用这种算法，主要的原因是它很难解决对象之间循环引用的问题。</p>

<h3 id="toc_2">可达性分析</h3>

<p>在主流的商用程序语言(Java、C#、LIsp)的主流实现都是通过可达性分析来判断对象是否存活的。可达性分析是从一系列的“GC Roots”对象作为起始点，开始向下搜索，搜索经过的路径叫做引用链，一个对象到GC Roots没有任何引用链相连，则说明这个对象是不可用的。<br/>
在Java语言中，可以作为GC Roots的对象有以下几种：<br/>
- 虚拟机栈（栈帧中的本地变量表）中引用的对象<br/>
- 方法区中类静态变量引用的对象<br/>
- 方法区中常量引用的对象<br/>
- 本地方法栈中Native方法引用的对象</p>

<h3 id="toc_3">引用类型</h3>

<p>JDK1.2之前只有一种引用：强引用；在之后对引用的概念进行了扩充，将引用分为强引用(Strong Reference)、软引用(Soft Reference)、弱引用(Weak Reference)和虚引用(Phantom Reference)，引用强度依次减弱。<br/>
- 强引用，就是常用的普通赋值引用，只要有强引用在，垃圾收集器永远不会回收掉被引用的对象；<br/>
- 软引用，用来描述一些有用但是非必须可以回收的对象。系统会在发生内存溢出异常之前，将这些对象列入回收范围中进行第二次回收，如果本次内存回收还是没有足够的内存，才会抛出内存溢出异常。使用SoftReference类来实现软引用；<br/>
- 弱引用，强度比软引用更弱，被弱引用的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器开始工作时，无论是否有足够的内存，都会回收只有弱引用的对象。使用WeakReference来实现弱引用；<br/>
- 虚引用，最弱的一种引用关系，又被称为幽灵引用或者幻影引用。一个对象是否有虚引用完全不影响其生存时间，也无法通过虚引用来取得一个对象实例。虚引用唯一的目的是当这个对象被收集器回收时收到一个系统通知。使用PhantomReference实现虚引用。</p>

<h3 id="toc_4">finalize方法</h3>

<p>当对象为GC Roots不可达时不一定会被立刻销毁，需要至少经过两次标记过程才会真正被回收。对象被分析判断为不可用后会被第一次标记并且进行一次筛选，筛选的条件是对象是否有必要执行finalize方法，只有对象覆盖了finalize方法且虚拟机没有调用过其finalize方法才会被认为有必要执行，之后会将其放置在一个F-Queue队列中。虚拟机会启动一个低优先级的Finalizer线程去执行它，而且并不保证等待它运行结束。稍后虚拟机会对F-Queue队列中的对象进行第二次小规模的标记，如果对象执行了finalize中成功的被其他引用链上的对象引用，则在第二次标记中会被移出“即将回收”的集合。值得注意的时finalize只会被执行一次，对象不会被无限次的使用finalize逃脱被回收。</p>

<h3 id="toc_5">回收方法区</h3>

<p>方法区中也会进行垃圾收集，只是性价比一般比较低。永久代中的垃圾回收主要收集两部分内容：废弃的常量和无用的类。<br/>
判断类是否“无用的类”需要满足3个条件：<br/>
- 该类的所有实例都已经被回收<br/>
- 加载该类的ClassLoader已经被回收<br/>
- 该类对应的java.lang.Class对象没有被引用，无法在任何地方通过反射访问该类的方法<br/>
在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGi这类频繁自定义ClassLoader的场景都需要虚拟机开启类卸载功能，以保证永久代不会溢出。</p>

<h2 id="toc_6">垃圾收集算法</h2>

<h3 id="toc_7">标记-清除算法</h3>

<p>算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。这是最基础的收集算法，后续的算法都是在这种思路的基础上进行改进得到的。它主要有两点不足：一个是效率问题，标记和清除两个过程效率都不高；另外一个是空间问题，标记清除之后产生大量不连续的内存碎片。</p>

<h3 id="toc_8">复制算法</h3>

<p>复制算法将可用的内存按容量划分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还存活的对象复制到另外一块上去，然后将已使用过的一块内存一次性清除，这样每次只需对半个区域的内存进行回收，避免了内存碎片的情况，只需要按序分配内存。实现简单，运行高效。只是这种算法只能使用一半内存，成本太高。<br/>
现在的主流虚拟机都使用这种算法来回收新生代，将新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和一块Survivor空间，当回收时，将Eden和Survivor中还存活的对象一次性复制到另外一块Survivor中，一次性清理Eden和Survivor空间。HotSpot中默认Eden和Survivor的比例为8：1，只有10%的空间浪费。当Survivor内存不够用是，需要依赖年老代进行分配担保。</p>

<h3 id="toc_9">标记-整理算法</h3>

<p>复制收集算法在对象存活率较高的情况下需要进行较多的复制操作，效率会变低，更关键的需要占用双倍的内存，因此在老年代使用这种算法并不合适。<br/>
根据老年代的特点出现了标记-整理算法，标记过程与标记-清除算法相同，标记完成后并不直接清理可回收对象，而是整理内存让所有存活对象都移动到内存的一端，然后清理边界之外的内存。</p>

<h3 id="toc_10">分代收集算法</h3>

<p>当前商业虚拟机的垃圾收集都采用了分代算法，根据对象的存活时间不同将内存分成几块，一般分为新生代和老年代，然后根据各个年代不同的特点在不同的区域使用不同的收集算法。<br/>
<img src="media/15212852694452/15214323424019.jpg" alt=""/></p>

<h2 id="toc_11">HotSpot的算法实现</h2>

<h3 id="toc_12">枚举根节点</h3>

<h3 id="toc_13">安全点</h3>

<h3 id="toc_14">安全区域</h3>

<h2 id="toc_15">垃圾收集器</h2>

<p><img src="media/15212852694452/15213810214661.jpg" alt=""/><br/>
上图展示了7种作用于不同分代的收集器，如果两个收集器之间有连线，就说明它们可以搭配使用。</p>

<h3 id="toc_16">Serial收集器</h3>

<p>Serial收集器是一个单线程的新生代收集器，使用复制算法。它进行垃圾收集时，必须暂停所有的用户线程。是Jvm client模式下默认的新生代收集器。在单个CPU的环境下，Serial收集器没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。<br/>
<img src="media/15212852694452/15214425876969.jpg" alt=""/><br/>
Serial/Serial Old收集器运行示意图</p>

<h3 id="toc_17">ParNew(Parallel New)收集器</h3>

<p>ParNew收集器是Serial收集器的多线程版本，除了使用多个线程进行垃圾收集之外，其他行为包括Serial收集器可用的所有控制参数、收集算法、Stop the World、对象分配规则、回收策略等都与Serial收集器完全一样。<br/>
ParNew收集器是唯一个可以与CMS收集器配合的多线程新生代收集算法，指定使用CMS收集器时，默认的新生代收集器也是ParNew。<br/>
在单CPU环境下ParNew的效率不如Serial，随着CPU数量的增多，效率才会高于Serial。<br/>
<img src="media/15212852694452/15214438006546.jpg" alt=""/><br/>
ParNew/Serial Old收集器运行示意图</p>

<h3 id="toc_18">Parallel Scavenge收集器(吞吐量优先收集器)</h3>

<p>Parallel Scavenge收集器也是一个并发多线程的新生代收集器，也是使用的复制算法。特点是关注点与其他收集器不同，CMS等收集器关注的是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge的目标是达到一个可控制的吞吐量，吞吐量=程序运行时间/(程序运行时间+垃圾收集时间)</p>

<h3 id="toc_19">Serial Old收集器</h3>

<p>Serial的老年代版本，单线程收集使用“标记-整理”算法，主要在JVM client模式下使用。</p>

<h3 id="toc_20">Parallel Old收集器</h3>

<p>Parallel Old是Parallel Scavenge收集器的老年代版本，使用并发多线程收集用“标记-整理”算法实现。<br/>
<img src="media/15212852694452/15214452957714.jpg" alt="示意图"/><br/>
Parallel Scavenge/Parallel Old收集器运行示意图</p>

<h3 id="toc_21">CMS收集器</h3>

<p>CMS(Concurrent Mark Sweep)收集器是一种以获取最短的回收停顿时间为目的的收集器。CMS收集器基于“标记-清除”算法实现，整个收集过程大致分为四个阶段：<br/>
1. 初始标记(CMS init mark)<br/>
2. 并发标记(CMS concurrent mark)<br/>
3. 重新标记(CMS remark)<br/>
4. 并发清除(CMS concurrent sweep)<br/>
其中初始标记、重新标记两个阶段需要停顿其他用户进程。<br/>
初始标记仅仅只标记出GC Roots能够直接关联到的对象，速度很快；并发标记阶段是进行GC Roots根搜索算法阶段，会判断对象是否存活；重新标记阶段是为了修正并发标记期间，因用户进程继续执行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间会比初始标记阶段稍长，但运行时间比并发标记阶段要短。<br/>
由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以和用户线程一起工作，所以整体来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。<br/>
<img src="media/15212852694452/15214610749086.jpg" alt="Concurrent Mark Sweep 收集器运行示意图"/><br/>
Concurrent Mark Sweep 收集器运行示意图</p>

<p>CMS收集器的优点是并发收集、低停顿，但是存在三个明显的缺陷：<br/>
1. CMS收集器对CPU资源非常敏感。在并发阶段，虽然不会停顿用户线程，但是会占用CPU资源而导致应用程序变慢，总吞吐量下降。CMS默认启动的回收线程数是：(CPU数量+3)/4；<br/>
2. CMS无法处理浮动垃圾，可能出现“Concurrent Mode Failure”，失败后会导致另一次FullGC的产生。由于CMS并发清理阶段用户线程还在运行，伴随程序的运行自然会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在本次收集中处理它们，只能留到下次GC进行清理。这一部分垃圾被称为“浮动垃圾”。由于CMS收集器收集阶段用户线程还要运行，需要预留一部分内存空间提供给收集时的用户线程使用。在默认配置下，CMS收集器在老年代使用了68%的内存空间时就会被激活，可以通过-XX:CMSInitiatingOccupancyFraction的值来设置触发百分比。要是CMS运行期间预留的内存无法满足程序其他线程的需要，就会出现“Concurrent Mode Failure”失败，这时虚拟机会启动后备预案：临时启动Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就会很长了；<br/>
3. CMS是基于“标记-清除”算法实现的收集器，会产生大量的空间碎片。空间碎片太多时，会给对象分配带来麻烦，内存空间找不到足够的连续空间时会提前触发Full GC。为了解决这个问题，CMS收集器提供了一个-XX:UseCMSCompactAtFullCollection开关参数，用于在Full GC之后增加一个碎片整理的过程，还可以通过-XX:CMSFullGCBeforeCompaction参数设置执行多少次不压缩的Full GC之后，进行一次碎片整理过程。</p>

<h3 id="toc_22">G1(Gebage-First)收集器</h3>

<p>G1不再是只管理新生代或者老年代，而是将整个Java堆划分成多个大小相等的独立区域(Region)，虽然还有新生代和老年代的区别，但不同代之间不再是物理隔离的，它们都是一部分Region的集合。<br/>
G1通过跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先队列，每次根据允许的收集时间，优先回收价值最大的Region。<br/>
在G1收集器中，Region之间的对象引用以及其他收集器中的新生代和老年代之间的对象引用，虚拟机都是使用Remembered Set来避免全堆扫描。G1中每个Region都有一个与之对应的Remembered Set，虚拟机发现程序在对Reference类型的数据进行写操作时，会产生一个Write Barrier暂时中断写操作，检查Reference引用的对象是否处于不同的Region之中，如果是，便通过CardTable把相关的引用信息记录到被引用对象所属的Region的Remembered Set之中。当进行内存回收时，在GC Roots的范围中加入Remembered Set即可保证不对全堆扫描也不会遗漏。<br/>
G1收集器的特点：<br/>
- 并行与并发：G1能够充分利用多CPU来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿的GC动作，G1收集器可以通过并发的方式让Java程序继续运行；<br/>
- 空间整合：G1整体上看是基于“标记-整理”算法实现，从局部(两个Region之间)上看是基于“复制”算法实现的，避免了内存碎片的问题；<br/>
- 可预测的停顿：G1除了追求低停顿，还能建立可预测的停顿时间模型。</p>

<p>在G1中有一类特殊Region，叫Humongous区域，如果一个对象占用的空间超过分区容量的50%以上，G1收集器就会认为是一个巨型对象。这些巨型对象，默认直接会被分配在老年代。G1划分了Humongous区，用来专门存放巨型对象。如果一个H区装不下一个巨型对象，那么G1会寻找连续的H分区来存储。为了找到连续的H区，有可能会启动Full GC。</p>

<h4 id="toc_23">对象分配策略</h4>

<p>对象分配分为3个阶段：<br/>
1. TLAB(Thread Local Allocation Buffer)线程本地分配缓冲区<br/>
2. Eden区分配<br/>
3. Humongous区分配<br/>
TLAB的目的是为了是提高对象内存分配的效率，如果对象在线程共享内存区域分配，需要采用同步机制来管理这些空间的空闲内存指针。在Eden空间中，每个线程都有一个固定的分区用来分配对象，即TLAB。这样分配对象时，就不需要同步机制了。<br/>
对TLAB空间中无法分配的对象，JVM会尝试在Eden空间中进行分配。如果Eden空间无法容纳，就只能在老年代中进行分配。</p>

<h4 id="toc_24">G1的两种GC模式</h4>

<p>G1提供了两种GC模式，Young GC和Mixed GC，都是STW的。</p>

<h5 id="toc_25">G1 Young GC</h5>

<p>Young GC分为5个阶段<br/>
1. 根扫描：静态和本地对象被扫描<br/>
2. 更新Remembered Set：处理dirty card队列更新Remembered Set<br/>
3. 处理Remembered Set：检测从年轻代指向老年代的对象<br/>
4. 对象拷贝：拷贝存活的对象到Survivor/Old区<br/>
5. 处理引用队列：软引用、弱引用、虚引用处理</p>

<h5 id="toc_26">G1 Mixed GC</h5>

<p>Mixed GC分为两个阶段<br/>
1. 全局并发标记(Global concurrent marking)<br/>
2. 拷贝存活对象(evacuation)</p>

<h4 id="toc_27">G1的收集过程</h4>

<p>G1的收集过程大致可以分为4个阶段<br/>
1. 初始标记(STW)：对GC Roots能直接关联的对象进行标记，且修改TAMS(Next Top at Mark Start)的值(Java 10由单线程改为多线程)。<br/>
2. 并发标记：从GC Roots开始对整个堆进行分析，整个阶段用时较长，可以与用户线程并发执行；<br/>
3. 最终标记：将上阶段中产生的Remembered Set Logs与Remembered Set合并，完成标记。该阶段会STW，但是可以并行执行。<br/>
4. 筛选回收(STW)：对各个Region的回收价值和成本进行排序，根据用户期待的GC停顿时间来执行回收计划。G1 会识别完全空闲区域和可供进行Mixed<br/>
GC 的区域，清理阶段将空白区域重置并返回到空闲列表时为部分并发。<br/>
<img src="media/15212852694452/15216267188183.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[梳理一下自己的知识体系]]></title>
    <link href="https://www.rackyun.ml:8810/wiki/summary/15212852539539.html"/>
    <updated>2018-03-17T19:14:13+08:00</updated>
    <id>https://www.rackyun.ml:8810/wiki/summary/15212852539539.html</id>
    <content type="html"><![CDATA[
<hr/>

<h3 id="toc_0">整理计划</h3>

<ol>
<li> <a href="15212921864702.html">Java内存区域</a></li>
<li> <a href="15212852694452.html">GC算法知识整理</a></li>
</ol>

]]></content>
  </entry>
  
</feed>
